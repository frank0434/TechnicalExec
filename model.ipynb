{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing required values\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import category_encoders as ce\n",
    "# importing libraries\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import  RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "%matplotlib inline\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the train data\n",
    "data = pd.read_csv('Data\\BigMart.csv')\n",
    "# check for the null values\n",
    "\n",
    "data.drop(\"Unnamed: 0\", axis = 1, inplace=True)\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data.Item_Weight.hist()\n",
    "data.Outlet_Size.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()\n",
    "df_ = data.select_dtypes(exclude=['int', 'float'])\n",
    "for col in df_.columns:\n",
    "    print(df_[col].unique()) # to print categories name only\n",
    "    # print(df_[col].value_counts()) # to print count of every category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix some irregular values \n",
    "data.loc[data['Item_Fat_Content'].isin(['LF','low fat']), 'Item_Fat_Content'] = 'Low Fat'\n",
    "data.loc[data['Item_Fat_Content'].isin(['reg']), 'Item_Fat_Content'] = 'Regular'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# impute missing values in item weight by mean\n",
    "data.Item_Weight.fillna(data.Item_Weight.mean(),inplace=True)\n",
    "# impute outlet size in training data by mode\n",
    "data.Outlet_Size.fillna(data.Outlet_Size.mode()[0],inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an object of the OneHotEncoder\n",
    "OHE = ce.OneHotEncoder(cols=['Item_Fat_Content',\n",
    "                             'Item_Type',\n",
    "                             'Outlet_Identifier',\n",
    "                             'Outlet_Size',\n",
    "                             'Outlet_Location_Type',\n",
    "                             'Outlet_Type'],use_cat_names=True)\n",
    "# encode the categorical variables\n",
    "data = OHE.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.isna().sum()\n",
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an object of the StandardScaler\n",
    "scaler = StandardScaler()\n",
    "# fit with the Item_MRP\n",
    "scaler.fit(np.array(data.Item_MRP).reshape(-1,1))\n",
    "scaler.fit(np.array(data.Item_Weight).reshape(-1,1))\n",
    "# transform the data\n",
    "data.Item_MRP = scaler.transform(np.array(data.Item_MRP).reshape(-1,1))\n",
    "data.Item_Weight = scaler.transform(np.array(data.Item_Weight).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the independent and target variable \n",
    "train_X = data.drop(columns=['Item_Identifier','Item_Outlet_Sales'])\n",
    "train_Y = data['Item_Outlet_Sales']\n",
    "\n",
    "# randomly split the data\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_X, train_Y,test_size=0.2,random_state=0)\n",
    "\n",
    "# shape of train and test splits\n",
    "train_x.shape, test_x.shape, train_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an object of the LinearRegression Model\n",
    "model_LR = LinearRegression()\n",
    "\n",
    "# fit the model with the training data\n",
    "model_LR.fit(train_x, train_y)\n",
    "\n",
    "# predict the target on train and test data \n",
    "predict_train = model_LR.predict(train_x)\n",
    "predict_test  = model_LR.predict(test_x)\n",
    "\n",
    "# Root Mean Squared Error on train and test date\n",
    "print('RMSE on train data: ', mean_squared_error(train_y, predict_train)**(0.5))\n",
    "print('RMSE on test data: ',  mean_squared_error(test_y, predict_test)**(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an object of the RandomForestRegressor\n",
    "model_RFR = RandomForestRegressor(max_depth=10)\n",
    "\n",
    "# fit the model with the training data\n",
    "model_RFR.fit(train_x, train_y)\n",
    "\n",
    "# predict the target on train and test data\n",
    "predict_train = model_RFR.predict(train_x)\n",
    "predict_test = model_RFR.predict(test_x)\n",
    "\n",
    "# Root Mean Squared Error on train and test data\n",
    "print('RMSE on train data: ', mean_squared_error(train_y, predict_train)**(0.5))\n",
    "print('RMSE on test data: ',  mean_squared_error(test_y, predict_test)**(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RF is better than LR so far."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try a lasso regression to drop some features?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "regressor = linear_model.Lasso(alpha=100,\n",
    "                               positive=True,\n",
    "                               fit_intercept=False, \n",
    "                               max_iter=1000,\n",
    "                               tol=0.0001)\n",
    "regressor.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import eli5\n",
    "eli5.show_weights(regressor, top=-1, feature_names = train_x.columns.tolist())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three features lead the way? \n",
    "Not sure if it's going to work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# separate the independent and target variable \n",
    "train_X = data.loc[:,['Item_MRP', 'Outlet_Identifier_OUT027', 'Outlet_Type_Supermarket Type1']]\n",
    "train_Y = data['Item_Outlet_Sales']\n",
    "\n",
    "# randomly split the data\n",
    "train_x, test_x, train_y, test_y = train_test_split(train_X, train_Y,test_size=0.2,random_state=0)\n",
    "\n",
    "# shape of train and test splits\n",
    "train_x.shape, test_x.shape, train_y.shape, test_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an object of the LinearRegression Model\n",
    "model_LR = LinearRegression()\n",
    "\n",
    "# fit the model with the training data\n",
    "model_LR.fit(train_x, train_y)\n",
    "\n",
    "# predict the target on train and test data \n",
    "predict_train = model_LR.predict(train_x)\n",
    "predict_test  = model_LR.predict(test_x)\n",
    "\n",
    "# Root Mean Squared Error on train and test date\n",
    "print('RMSE on train data: ', mean_squared_error(train_y, predict_train)**(0.5))\n",
    "print('RMSE on test data: ',  mean_squared_error(test_y, predict_test)**(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an object of the RandomForestRegressor\n",
    "model_RFR = RandomForestRegressor(max_depth=10)\n",
    "\n",
    "# fit the model with the training data\n",
    "model_RFR.fit(train_x, train_y)\n",
    "\n",
    "# predict the target on train and test data\n",
    "predict_train = model_RFR.predict(train_x)\n",
    "predict_test = model_RFR.predict(test_x)\n",
    "\n",
    "# Root Mean Squared Error on train and test data\n",
    "print('RMSE on train data: ', mean_squared_error(train_y, predict_train)**(0.5))\n",
    "print('RMSE on test data: ',  mean_squared_error(test_y, predict_test)**(0.5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting worse, maybe try some other models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.select_dtypes(include=['int', 'float']).columns\n",
    "# data.dtypes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import Imputer\n",
    "\n",
    "imputer = Imputer(strategy=\"median\")\n",
    "\n",
    "num_pipeline = Pipeline([\n",
    "        (\"select_numeric\", DataFrameSelector([\"SibSp\", \"Parch\", \"Fare\",'RelativesOnboard'])),\n",
    "        (\"imputer\", Imputer(strategy=\"median\")),\n",
    "        ('Scaler', StandardScaler())\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training data with 7 most important features\n",
    "train_x_if = train_x[['Item_MRP', \n",
    "                    'Outlet_Type_Grocery Store',\n",
    "                    'Item_Visibility',\n",
    "                    'Outlet_Type_Supermarket Type3',\n",
    "                    'Outlet_Identifier_OUT027',\n",
    "                    'Outlet_Establishment_Year',\n",
    "                    'Item_Weight']]\n",
    "# test data with 7 most important features\n",
    "test_x_if = test_x[['Item_MRP', \n",
    "                    'Outlet_Type_Grocery Store',\n",
    "                    'Item_Visibility',\n",
    "                    'Outlet_Type_Supermarket Type3',\n",
    "                    'Outlet_Identifier_OUT027',\n",
    "                    'Outlet_Establishment_Year',\n",
    "                    'Item_Weight']]\n",
    "\n",
    "# create an object of the RandfomForestRegressor Model\n",
    "model_RFR_with_if = RandomForestRegressor(max_depth=10,random_state=2)\n",
    "\n",
    "# fit the model with the training data\n",
    "model_RFR_with_if.fit(train_x_if, train_y)\n",
    "\n",
    "# predict the target on the training and test data\n",
    "predict_train_with_if = model_RFR_with_if.predict(train_x_if)\n",
    "predict_test_with_if = model_RFR_with_if.predict(test_x_if)\n",
    "\n",
    "# Root Mean Squared Error on the train and test data\n",
    "print('RMSE on train data: ', mean_squared_error(train_y, predict_train_with_if)**(0.5))\n",
    "print('RMSE on test data: ',  mean_squared_error(test_y, predict_test_with_if)**(0.5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the BaseEstimator\n",
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "# define the class OutletTypeEncoder\n",
    "# This will be our custom transformer that will create 3 new binary columns\n",
    "# custom transformer must have methods fit and transform\n",
    "class OutletTypeEncoder(BaseEstimator):\n",
    "\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def fit(self, documents, y=None):\n",
    "        return self\n",
    "\n",
    "    def transform(self, x_dataset):\n",
    "        x_dataset['outlet_grocery_store'] = (x_dataset['Outlet_Type'] == 'Grocery Store')*1\n",
    "        x_dataset['outlet_supermarket_3'] = (x_dataset['Outlet_Type'] == 'Supermarket Type3')*1\n",
    "        x_dataset['outlet_identifier_OUT027'] = (x_dataset['Outlet_Identifier'] == 'OUT027')*1\n",
    "        \n",
    "        return x_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pre-processsing step\n",
    "# Drop the columns - \n",
    "# Impute the missing values in column Item_Weight by mean\n",
    "# Scale the data in the column Item_MRP\n",
    "pre_process = ColumnTransformer(remainder='passthrough',\n",
    "                                transformers=[('drop_columns', 'drop', ['Item_Identifier',\n",
    "                                                                        'Outlet_Identifier',\n",
    "                                                                        'Item_Fat_Content',\n",
    "                                                                        'Item_Type',\n",
    "                                                                        'Outlet_Identifier',\n",
    "                                                                        'Outlet_Size',\n",
    "                                                                        'Outlet_Location_Type',\n",
    "                                                                        'Outlet_Type'\n",
    "                                                                       ]),\n",
    "                                              ('impute_item_weight', SimpleImputer(strategy='mean'), ['Item_Weight']),\n",
    "                                              ('scale_data', StandardScaler(),['Item_MRP'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Pipeline\n",
    "\"\"\"\n",
    "Step1: get the oultet binary columns\n",
    "Step2: pre processing\n",
    "Step3: Train a Random Forest Model\n",
    "\"\"\"\n",
    "model_pipeline = Pipeline(steps=[('get_outlet_binary_columns', OutletTypeEncoder()), \n",
    "                                 ('pre_processing',pre_process),\n",
    "                                 ('random_forest', RandomForestRegressor(max_depth=10,random_state=2))\n",
    "                                 ])\n",
    "# fit the pipeline with the training data\n",
    "model_pipeline.fit(train_x,train_y)\n",
    "\n",
    "# predict target values on the training data\n",
    "model_pipeline.predict(train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# read the test data\n",
    "test_data = pd.read_csv('dataset/test_t02dQwI.csv')\n",
    "\n",
    "# predict target variables on the test data \n",
    "model_pipeline.predict(test_data)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "513dc2e41d739bb2c947903f3c0bbf636d03aa53ab50e61c694a27481c81805e"
  },
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('base': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
